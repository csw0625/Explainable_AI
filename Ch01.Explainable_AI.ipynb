{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainable AI : XAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "설명가능한 AI(XAI)는 AI 모형이 만들어 낸 예측치에 대한 근거(rationale)을 이해하기 위한 방법론으로, end-user가 이해할 수 있느냐에 따라 AI 모형과 XAI로 구분이 된다. AI 모형은 일반적으로 결과를 도출해 내기는 하지만 결과 생성 과정을 이해할 수 없는 모형이며 이를 **Black-Box** 모형이라고 한다. 한편, XAI는 AI 모형의 사용자가 모형을 이해할 수 있도록 하고 모형의 예측 결과를 신뢰할 수 있는지를 판단하게 하는 방법론이다.\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20231201153509/Explainable-AI-Concept-1-660.png\" alt=\"이미지 설명\" />\n",
    "  <p style=\"font-size: 12px; color: grey; text-align: center;\"><em>출처: https://media.geeksforgeeks.org/wp-content/uploads/20231201153509/Explainable-AI-Concept-1-660.png</em></p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XAI는 다음과 같은 AI 모형에 의한 의사결정에 요인들을 점검할 수 있게 해준다.\n",
    "- **기대효과**\n",
    "- **투명성**\n",
    "- **편향(bias)**\n",
    "- **공정성(fairness)**  \n",
    "\n",
    "**투명성**은 특정 변수가 어떻게 얼마만큼 어떤 형태로 예측치에 영향을 미치는가를 알려주는 척도이다.  \n",
    "\n",
    "**편향**은 예를 들어 암환자 확진 여부를 예측하는 모형에서 특정 연령대에 있는 사람이 모두 암이며, 표본이 작을 경우 특정 연령대의 암 진단에 대한 연령의 효과가 다른 연령대에 비해 매우 높게 나오는 현상을 말한다.  \n",
    "\n",
    "  **공정성**은 사진으로 의사(doctor) 여부를 딥러닝 모형으로 분류하고자 할 때, 데이터가 대부분 가운을 입고 청진기를 목에 두른 백인 남성 이미지이면 백인 남성이 의사(doctor) 여부를 결정하는 중요한 요소가 된다. 즉, 여성이거나 다른 유색인종이 청진기를 목에 두른 이미지는 의사가 아니라고 구분할 가능성이 높아진다. 이런 경우 해당 딥러닝 모형이 공정성을 갖지 못한다고 말한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델의 해석력과 성능 간의 Trade-off 관계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선형모형이나 로지스틱 회귀모형은 복잡한 앙상블러닝이나 딥러닝 모형에 비해 성능이 떨어지지만 여전히 빈번하게 사용되는 회귀 및 분류모형이다. 근본적으로 해석이 용이하고 예측치에 대한 특성 변수의 기여도가 명확한 **해석가능(Interpretable)** 모형이기 때문이다. 이러한 모델들을 **White-Box** 혹은 **Glass-Box** 모형이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y_i = \\beta_{0} + \\beta_{1}x_{1i} + \\dots + \\beta_{p}x_{pi}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 예시와 같은 선형회귀모형에서 $\\beta_{k}x_{ki}$ 는 $i$번째 표본의 변수 $x_{ki}$ 가 예측치에 영향을 주는 정도를 수량화한 것이고, $\\beta_{k}$는 절대값의 크기로 목적 변수, 혹은 예측변수 $y$에 기여하는 중요도를 설명하며, 부호로 예측치의 증가 및 감소를 평가하는 방향성 정보도 제공한다. 이렇게 모형식 자체로부터 예측치가 어떻게 생성되는지를 알 수 있는 모형이 해석가능한 모형이다.  \n",
    "이런 수량화는 맞게 예측한 경우와 틀리게 예측한 경우를 나누어 그 이유를 설명할 수 있게 해주며 이를 통해 모형을 실제 문제에 적용할 수 있는지에 대한 모형의 신뢰도 지표로도 사용할 수 있다.  \n",
    "**해석 가능한 모델(Interpretable Model)** 은 먼저 근본적으로 **형태를 가정하고** 모델링을 실시한다. 선형회귀식은 말그대로 설명 변수들과 예측 변수 간 **선형적** 인 관계를 가정이 기저에 깔려 있으며, 그 외 다른 가정들도 수반한다. 이런 가정 하에서 우리는 해석이 가능한 것이며, 대부분의 가정들이 만족을 하면 선형 회귀식도 충분히 좋은 성능을 보장할 수 있다.  \n",
    "선형회귀 모형이나 로지스틱 모형들을 **Parametric 모델**이라고도하며, $\\beta$와 같은 **모수(parameter)** 들을 통해 우리는 모델 자체에 대한 해석이 가능한 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만 이러한 모형의 신뢰도는 모형의 성능과 일치하지 않는다. 위와 같이 해석가능한 모형들은 **많은 가정들**을 필요로 하며, 가정이 위반되는 순간 모형의 성능 또한 저하가 되며 모수의 해석에 오류가 발생할 수 있다. 이와 반대로 대표적인 Black-Box 모형인 Bagging 또는 Boosting 모형과 같은 앙상블 기법이나, 수많은 은닉층으로 구성된 딥러닝의 예측 정밀도는 기존의 선형모형보다 월등하며 모형이나, 데이터에 대한 가정으로부터 매우 자유롭다.  \n",
    "하지만 내부적으로 어떻게 작동하여 예측치가 만들어지는지 알 수 없으며 이들 모형들의 **신뢰도**는 일반적으로 낮은 편이다.\n",
    "<div style=\"text-align: center; color: gray;\">하지만 Black-Box 모형이 항상 뛰어난 성능을 발휘하는 건 아니며, 때론 데이터에 따라 해석가능한 모형들이 더 뛰어난 성능을 보이는 경우도 있다. </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"https://datascientest.com/wp-content/uploads/2020/05/Capture-d%E2%80%99e%CC%81cran-2020-05-20-a%CC%80-13.02.44-1024x492.png\" alt=\"이미지 설명\" />\n",
    "  <p style=\"font-size: 12px; color: grey; text-align: center;\"><em>출처: https://datascientest.com/wp-content/uploads/2020/05/Capture-d%E2%80%99e%CC%81cran-2020-05-20-a%CC%80-13.02.44-1024x492.png</em></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "유리박스 모형과 블랙박스 모형을 모형의 복잡한 정도로 구분하는 것이 일반적이지만, 모형의 **미분 가능성**으로 분류하기도 한다. 예를 들어, 딥러닝 모형을 tensorflow.keras로 구축하면 tensorflow의 자동 미분 기능으로 인해 모형 내부의 모든 모수에 대해 미분 가능하므로 유리박스 모형으로 분류되지만, Bagging 또는 Boosting 모형은 미분이 불가능하므로 블랙박스 모형으로 구분된다.  \n",
    "이러한 분류는 모형의 설명 가능성(explainability)를 구할 때 머신러닝 모형의 미분 여부가 중요한 역할을 하기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid rgb(233, 224, 222); padding: 10px; background-color: rgb(233, 224, 222); margin: 10px 0;\">\n",
    "    <strong style=\"font-weight: bold;\">Interpretable VS Explainable</strong><br>\n",
    "    <b>해석 가능(Interpretable)</b>과 <b>설명 가능(Explainable)</b>은 같은 뜻이 아니다.\n",
    " 이 두 용어의 차이는 주로 모델의 복잡성과 그 모델이 제공하는 인사이트의 종류에 따라 달라진다. <br><br>\n",
    "    <strong style=\"font-weight: bold;\">● Interpretable (해석 가능)</strong><br>\n",
    "    Interpretable은 모델이 사람의 직관에 맞게 쉽게 이해될 수 있다는 뜻이다. 즉, 모델의 내부 동작이나 결정을 인간이 직관적으로 이해할 수 있을 때 해당 모델을 \"해석 가능\"하다고 한다. <br>\n",
    "    - <strong>직관적인 이해</strong>: 모델의 작동 원리나 결정 과정을 이해하는 데 직관적이고 단순한 모델일수록 해석 가능하다. <br>\n",
    "    - <strong>모델 자체의 단순성</strong>: 결정 트리(Decision Tree), 선형 회귀(Linear Regression), 로지스틱 회귀(Logistic Regression)와 같은 간단한 모델들이 일반적으로 해석 가능하다고 여겨진다. 이러한 모델들은 결과를 도출하는 과정이 명확하고, 쉽게 설명할 수 있기 때문이다. <br>\n",
    "    - <strong>설명 가능한 이유</strong>: 각 입력 변수(특징)가 모델의 예측에 어떻게 기여하는지 쉽게 알 수 있다. <br><br>\n",
    "    <strong style=\"font-weight: bold;\">● Explainable (설명 가능)</strong><br>\n",
    "    Explainable은 모델의 예측이나 결정을 사람이 이해할 수 있는 방식으로 설명할 수 있다는 뜻이다. 즉, 모델이 복잡하거나 그 내부를 직관적으로 이해하기 어려운 경우에도, 그 예측이 어떻게 이루어졌는지 설명할 수 있는 능력을 말한다. <br>\n",
    "    - <strong>모델의 예측을 설명할 수 있는가?</strong>: \"설명 가능\"은 모델이 복잡하고 그 내부를 직접적으로 이해하기 어려운 경우에도, 그 예측을 도출한 이유를 설명할 수 있는 능력을 말한다. <br>\n",
    "    - <strong>복잡한 모델에 적용</strong>: 딥러닝(Deep Learning)이나 앙상블(Ensemble) 모델처럼 복잡하고 블랙박스(Black-box) 형태인 모델들도 설명 가능할 수 있다. 이 경우, 모델의 내부 구조는 복잡하지만, 예측이 어떻게 이루어졌는지 설명할 수 있는 기술이 필요하다. <br>\n",
    "    - 예를 들어, 추후 배울 SHAP(Shapley Additive Explanations)이나 LIME(Local Interpretable Model-Agnostic Explanations)와 같은 기법들이 복잡한 모델의 예측을 설명하는 데 사용된다. <br>\n",
    "    - <strong>모델의 설명</strong>: 모델의 예측을 이해할 수 있도록 도와주는 도구나 기법이 필요하다. 이때 \"설명\"은 사람이 이해할 수 있는 언어로 결과를 전달하는 것이다.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainer의 종류\n",
    "Explainer는 크게 **Global Explainer** 와 **Local Explainer** 로 나눌 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ● Global Explainer  \n",
    "**Global Explainer** 는 전체 데이터에서 차지하는 설명변수의 중요도를 파악하는 Explainer를 말한다. 앞서 선형회귀모형에서 $\\beta_{k}$ 는 $k$번째 설명변수의 global explainer 라고 할 수 있다.  \n",
    "대표적인 global explainer로는 PD(Partial dependence), PV(Partial variance), PI(Permutation importance), ALE(Accumulated Local Effect) 등이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ● Local Explainer\n",
    "**Local Explainer** 는 개별 표본에 대해 특성 변수가 예측치에 미치는 기여도를 측정하는 Explainer를 말한다. 회귀모형의 예시에서 $\\beta_{k}x_{ki}$ 는 $i$번쨰 표본의 설명변수 $x_{ki}$ 값에 따라 다르게 영향을 주므로 local explainer에서 속한다.  \n",
    "대표적인 local explainer로 SHAP(Sharpley Additive Explainer), LIME(Local intepretable Model-agnostic Explainer), Anchors, Counterfactual instances, IG(Integrated Gradients), SE(Similarity Explainer) 등이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 SHAP은 **local explainer 면서 동시에 global explainer이다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 explainer들을 실행하고 시각화하는 오픈소스 라이브러리는 대표적으로 ELI5, shap, alibi, lime 등이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.ELI5\n",
    "**ELI5**는 sklearn에서 제공하는 머신러닝 모형의 설명 가능성과 관련된 수치를 하나의 라이브러리로 통합한 것이다. ELI5는 global과 local explainer 모두 제공하지만 sklearn에서 제공하는 tabular 데이터에서만 적용되며, 앙상블러닝과 같이 복잡한 모형은 오직 global explainerdls PI만 제공한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.LIME\n",
    "**LIME**은 블랙박스 모형에 적용되는 모형 무관(Model-agnostic) local explainer이다. LIME은 tabular 데이터에도 적용이 가능하지만 하이퍼파라미터 선정 문제 때문에 자연어나 이미지 데이터의 explainer로 사용하는것이 권장된다. LIME은 **대리모형(Surrogate model)** 을 이용용하여 블랙박스 모형에서의 설명변수 기여도를 계산한다.  \n",
    "이미지 자료의 예시로, 이미지를 픽셀들의 모임인 super-pixel로 나눈 후, 이 super-pixel을 **on/off의 개념**으로 원래의 이미지에서 임의로 제거한다. 만약 off된 super-pixel이 중요한 픽셀의 모임이면 블랙박스 모형의 성능이 많이 떨어질 것이고, 반대로 기여도가 적다면 제거하여도 모델 성능에 거의 영향이 없을 것이다. 이런 변화를 대리모형으로 탐지하는 것이 LIME의 작동원리이다.  \n",
    "LIME에서 특성 변수의 on/off의 개념은 tabular, 이미지, 텍스트에 따라 다르게 정의되기 때문에 LIME 라이브러리는 Limetabular, LimeText, 그리고 LimeImage explainer로 구분하여 제공한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid rgb(233,224,222); padding: 10px; background-color: rgb(233,224,222); margin: 10px 0;\">\n",
    "    <strong style=\"font-weight: bold;\">Surrogate Model</strong><br>\n",
    "    <b>대리 모델(Surrogate Model)<b>은 실제 모델이 너무 복잡하거나 계산 비용이 많이 드는 경우, 그 모델을 대신하여 예측이나 분석을 수행할 수 있는 간단한 모델이다. 대리 모델은 원래 모델의 동작을 근사하여 효율성을 높이고, 학습이나 최적화 과정에서 시간과 자원을 절약할 수 있다. <br><br>\n",
    "    <strong style=\"font-weight: bold;\">특징</strong><br>\n",
    "    - <strong>근사 모델</strong>: 대리 모델은 원래 모델의 복잡한 계산을 단순화하여 근사할 수 있는 모델이다. 실제 모델이 비효율적일 때 이를 대신해 예측을 수행한다. <br>\n",
    "    - <strong>학습 비용 절감</strong>: 복잡한 모델을 학습하거나 최적화할 때, 대리 모델을 사용하여 실험과 계산을 더 효율적으로 할 수 있다. <br>\n",
    "    - <strong>다양한 분야에서 활용</strong>: 대리 모델은 최적화, 모델링 등 여러 분야에서 사용된다. 예를 들어, 베이지안 최적화(Bayesian Optimization)에서는 실제 비용이 비싼 함수를 최적화할 때 대리 모델을 사용한다. <br><br>\n",
    "    <strong style=\"font-weight: bold;\">대리 모델의 예</strong><br>\n",
    "    - <strong>선형 회귀(Linear Regression)</strong>: 실제 모델이 비선형적이고 계산이 복잡할 때, 선형 회귀를 대리 모델로 사용하여 예측을 근사할 수 있다. <br>\n",
    "    - <strong>가우시안 프로세스(Gaussian Process)</strong>: 베이지안 최적화와 같은 분야에서 사용되는 대리 모델로, 복잡한 함수나 목표 함수를 근사하는 데 유용하다. <br>\n",
    "    - <strong>결정 트리(Decision Tree)</strong>: 복잡한 모델을 대체할 수 있는 간단한 대리 모델로, 예측 결과를 더 직관적으로 해석할 수 있게 한다. <br><br>\n",
    "    <strong style=\"font-weight: bold;\">활용 예시</strong><br>\n",
    "    - <strong>베이지안 최적화(Bayesian Optimization)</strong>: 실제 함수는 계산 비용이 크지만, 대리 모델을 통해 목표 함수의 동작을 근사하고 최적값을 찾는 과정에서 사용된다. <br>\n",
    "    - <strong>모델링</strong>: 복잡한 시스템을 완벽하게 모델링하기 어려운 경우, 대리 모델을 사용하여 시스템을 근사할 수 있다. <br>\n",
    "    - <strong>해석 가능한 모델로 대체</strong>: 신경망과 같은 복잡한 모델을 해석 가능한 모델로 근사하기 위해 대리 모델을 사용할 수 있다.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.alibi\n",
    "**ailbi**는 통합라이브러리로 해당 라이브러리를 통해 local 및 global explainer를 쉽게 구현할 수 있다. tabular,자연어,이미지 등 모든 종류의 데이터에 적용되지만 특히 **Anchors**나 **Counterfactual**을 유연하게 실행할 수 있게 만든 라이브러리이다.  \n",
    "\n",
    "**Anchors**는 특정 설명변수값이 특정 범위 내에 있으면 머신러닝 모형의 예측값이 거의 변동이 없는 핵심 설명변수와 해당 설명변수값의 범위(또는 범주)를 탐색하는 local explainer이다. tabular 데이터의 경우, 설명 변수를 범위로 범주화하고, 이미지의 경우 super-pixel로 범주화한다. 이렇게 범주화된 핵심 설명변수를 탐색하고 이 설명변수값이 주어졌을 때, 동일한 예측값을 출력할 확률과 전체 데이터에서 이러한 핵심 설명변수를 가진 비율(coverage)을 제시하여 explainer의 신뢰도를 제공한다.  \n",
    "\n",
    "**Counterfactual**은 어떤 설명변수를 얼마나 수정해야 원래 label을 다른 label로 변경할 수 있나?에 대한 해결책을 주는 local explainer이다. 예를 들어 대출 신청이 거부되었을 때, 대출신청자의 심사항목 중 어느 항목을 얼마나 개선해야 대출 신청이 승인되는지를 알고 싶을 때를 탐색하는 것이 counterfactual의 목적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.shap\n",
    "**shap**은 경제학자 Shapley의 게임이론을 근간으로 설명변수의 주 효과와 교호작용 효과가 예측값이 기여하는 정도를 측정하는 SHAP value를 구하고 시각화를 지원하는 라이브러리이다. 다양한 plot들을 제공하며 모형의 설명 가능성을 시각화하여 모형의 해석과 투명성,편향,fairness를 진단할 수 있도록 지원한다.  \n",
    "local explainer로서 SHAP value를 제공한 후, 이를 종합하여 global explainer의 SHAP VALUE를 구하는 구조로 되어 있다. Explainer 중 가장 널리 사용되고 있다.  \n",
    "\n",
    "shap 라이브러리에는 ShapExplainer, KernelShapExplainer, TreeShapExplainer, DeepShapExplainer가 있다.  \n",
    "ShapExplainer,KernelShapExplainer는 블랙박스 모형에 적용되므로 모형 무관(Model-Agnostic) Explainer이다. TreeShap은 Decision Tree를 기반으로 하는 Bagging이나 Boosting 모형에 특화된 explainer이고 DeepShapExplainer는 딥러닝 모형에 특화된 explainer이다.  \n",
    "\n",
    "ShapExplainer의 계산 부담을 줄이기 위해 KernelShap이 제안되었고 LIME과 같이 대리모형을 이용한다. TreeShap과 DeepShap은 적용되는 모형의 특성을 반영하여 계산부담을 최소화하기 위해 제안되었다.  \n",
    "Shap 라이브러리는 각 explainer에 따라 Shap Value의 자료타입,출력형태와 시각화 plot의 종류와 적용방법이 다르므로 각 explainer의 사용법을 잘 구분해야 한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.EBM\n",
    "**EBM(Explainable Boosting Machine)** 은 bagging과 boosting을 결합한 일반화 선형결합 모형(Generalized Additive Model,GAM)으로 해석가능 모델이다. Gradient Boosting을 기반으로 설계되었으므로 XGBoost,LightGBM 만큼의 성능을 가지고 있으면서 유리박스 모형이므로 모델 자체의 수학적 표현 자체가 global explainer면서 local explainer이다. 위에서 소개한 XAI가 기존 모델에 의존하여 explainer를 산출한 반면, EBM은 모형 자체가 explainer가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞으로 배울 XAI에 대해 표로 정리하면 다음과 같다.\n",
    "\n",
    "<center>\n",
    "\n",
    "| XAI  | Applicable Models(Black/White) | Explainer Type (Global/Local) | Task (Regression/Classification) | Data Type (Tabular/Text/Image) | Library | Etc |\n",
    "|:----:|:------------------------:|:-----------------------------:|:-------------------------------:|:-----------------------------:|:-------:|:---:|\n",
    "| **ALE**  | Black-box                | Global                        | Regression+Classification       | Continuous Tabular            | alibi   | 설명변수 간 독립성 가정 없음 |\n",
    "| **PD**   | Black-box                | Global                        | Regression+Classification       | Tabular                       | alibi   | 설명변수 간 독립성 가정 필요 |\n",
    "| **PV**   | Black-box                | Global                        | Regression+Classification       | Tabular                       | alibi   | PD에 의존                 | \n",
    "| **PI**   | Black-box                | Global                        | Regression+Classification       | Tabular                       | alibi,ELI5   | 손실함수와 목적 변수값이 필요 | \n",
    "| **LIME** | Black-box                | Local                         | Regression+Classification       | Tabular,Text,Image            | lime    |                          |\n",
    "| **Anchors**   | Black-box           | Local                         | Classification                  | Tabular,Text,Image            | alibi   | 계산 시간이 오래 걸림       |\n",
    "| **SHAP** | Black & White box        | Global & Local                | Regression+Classification       | Tabular,Text,Image            | shap    |                          |\n",
    "| **CEM**  | Black & White box        | Local                         | Classification                  | Tabular,Image                 | alibi   | autoencoder와 데이터 도메인 지식 필요 |\n",
    "| **IG**   | White-box                | Local                         | Regression+Classification       | Tabular,Text,Image            | alibi   | 미분 가능한 모델만 적용 가능 |\n",
    "| **CFI**  | Black & White box        | Local                         | Classification                  | Tabular,Image                 | alibi   | Hyperparameter tuning 필요|\n",
    "| **CFP**  | Black & White box        | Local                         | Classification                  | Tabular,Image                 | alibi   | autoencoder 필요          |\n",
    "| **CFRL** | Black-box                | Local                         | Classification                  | Tabular,Image                 | alibi   | 제한조건 가능, autoencoder 필요, 강화학습 모형 이용 |\n",
    "| **SE**   | White-box                | Local                         | Regression+Classification       | Tabular,Text,Image            | alibi   |                          |\n",
    "| **EBM**  |                          | Global & Local                | Regression+Classification       | Tabular                       | InterpretML | 모형 자제가 XAI        |\n",
    "\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 다양한 XAI 기법들이 있지만 가장 중요한 것은 어떤 Explainer를 사용하던지간에, 해석하고 싶은 **모델이 주어진 데이터를 잘 설명한다는 가정**이 필요하다. 이는 즉, 모델의 성능이 우선적으로 확보되어야 XAI의 결과가 의미있고 신뢰할 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
